{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from math import log,exp\n",
    "from matplotlib import style\n",
    "style.use('fivethirtyeight')\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "from math import pi\n",
    "import pickle\n",
    "\n",
    "def prob(mu,cov,x):\n",
    "    reg_cov = 1e-9*np.ones([len(cov)])\n",
    "    cov = np.add(cov, reg_cov)\n",
    "    diff = np.subtract(x,mu)\n",
    "    inside = 0.0\n",
    "    pro = 1\n",
    "    for i in range(len(cov)):\n",
    "        inside += pow(diff[i],2)/cov[i]\n",
    "        pro *= cov[i]\n",
    "    inside *= -0.5\n",
    "    new = pow(2*pi,len(cov))*pro\n",
    "    new = pow(new,0.5)\n",
    "    return (np.exp(inside)/(new*1.0))\n",
    "\n",
    "X = np.loadtxt(\"NLS_Group05/Class1.txt\")    \n",
    "newX = np.loadtxt(\"NLS_Group05/Class2.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class K_Means:\n",
    "    def __init__(self, k=4, tol=0.001, max_iter=30):\n",
    "        self.k = k\n",
    "        self.tol = tol\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "    def fit(self,data):\n",
    "        print \"Starting K-means....with\",self.k,\"clusters...\",self.max_iter,\"iterations\"\n",
    "\n",
    "        self.centroids = {}\n",
    "\n",
    "        for i in range(self.k):\n",
    "            self.centroids[i] = data[i]\n",
    "\n",
    "        for ii in range(self.max_iter):\n",
    "            \n",
    "            print ii,\n",
    "            self.classifications = {}\n",
    "\n",
    "            for i in range(self.k):\n",
    "                self.classifications[i] = []\n",
    "\n",
    "            for featureset in data:\n",
    "                distances = [np.linalg.norm(featureset-self.centroids[centroid]) for centroid in self.centroids]\n",
    "                classification = distances.index(min(distances))\n",
    "                self.classifications[classification].append(featureset)\n",
    "\n",
    "            prev_centroids = dict(self.centroids)\n",
    "            chom = []\n",
    "            for classification in self.classifications:\n",
    "                chom.append(len(self.classifications[classification]))\n",
    "                self.centroids[classification] = np.average(self.classifications[classification],axis=0)\n",
    "#                 print len(self.classifications[classification])\n",
    "            \n",
    "#             print chom\n",
    "            total_error = 0.0\n",
    "#             print len(self.classifications)\n",
    "            for c in self.centroids:\n",
    "                original_centroid = prev_centroids[c]\n",
    "                current_centroid = self.centroids[c]\n",
    "                for jj in self.classifications[c]:\n",
    "                    val1 = np.linalg.norm(jj-original_centroid)\n",
    "                    val2 = np.linalg.norm(jj-current_centroid)\n",
    "                    total_error += abs(val1-val2)\n",
    "#                     print val1,val2,total_error\n",
    "            \n",
    "#             print total_error\n",
    "            if (total_error<self.tol):\n",
    "                print \"K-means converged\"\n",
    "                break\n",
    "#             print self.cen/troids\n",
    "\n",
    "    def predict(self,data):\n",
    "        distances = [np.linalg.norm(data-self.centroids[centroid]) for centroid in self.centroids]\n",
    "        classification = distances.index(min(distances))\n",
    "        return classification\n",
    "\n",
    "class GMM:\n",
    "    def __init__(self,X,number_of_sources,iterations,tolerence):\n",
    "        self.iterations = iterations\n",
    "        self.number_of_sources = number_of_sources\n",
    "        self.X = X\n",
    "        self.mu = None\n",
    "        self.pi = None\n",
    "        self.cov = None\n",
    "        self.XY = None\n",
    "        self.tolerence = tolerence\n",
    "        self.N = len(self.X)\n",
    "        self.D = len(self.X[0])\n",
    "        self.log_likelihoods = [] \n",
    "    \n",
    "    \"\"\"Define a function which runs for iterations, iterations\"\"\"\n",
    "    def run(self):\n",
    "        \"\"\" 1. Set the initial mu, covariance and pi values\"\"\"\n",
    "        clf = K_Means(self.number_of_sources)\n",
    "        clf.fit(self.X)\n",
    "        self.mu = np.array(clf.centroids.values())\n",
    "        print \"K-means Complete....\"\n",
    "        \n",
    "        self.data_clusters = np.array(clf.classifications.values())\n",
    "        \n",
    "        self.cov = np.ones([self.number_of_sources,self.D])\n",
    "        \n",
    "        for cn in range(self.number_of_sources):    \n",
    "            covar=np.var(self.data_clusters[cn], axis=0)\n",
    "            for k in range(self.D):\n",
    "                self.cov[cn][k]=1.0*covar[k] \n",
    "                \n",
    "        self.pi = np.ones(self.number_of_sources)/self.number_of_sources # Are \"Fractions\"\n",
    "        for cn in range(self.number_of_sources):\n",
    "            self.pi[cn] = len(self.data_clusters[cn])/(self.N*1.0)\n",
    "            \n",
    "        self.gamma = np.zeros((len(self.X),self.number_of_sources))\n",
    "        \n",
    "        \n",
    "        \"\"\"Plot the initial state\"\"\"            \n",
    "        for ii in range(self.iterations): \n",
    "            print ii,\n",
    "            sum_denom = []\n",
    "#             print \"a\",\n",
    "            for i in range(self.N):\n",
    "                temp = 0.0\n",
    "                for j in range(self.number_of_sources):\n",
    "                    temp += self.pi[j]*prob(self.mu[j],self.cov[j],self.X[i])\n",
    "                sum_denom.append(temp)\n",
    "#             print \"b\",\n",
    "            for i in range(self.N):\n",
    "                for j in range(self.number_of_sources):\n",
    "                    self.gamma[i][j] = self.pi[j]*prob(self.mu[j],self.cov[j],self.X[i])\n",
    "                    if(sum_denom[i]>0):\n",
    "                        self.gamma[i][j] /= sum_denom[i] \n",
    "            \n",
    "#             print \"c\",\n",
    "            sum2 = []\n",
    "            for i in range(self.number_of_sources):\n",
    "                temp = 0.0\n",
    "                for j in range(self.N):\n",
    "                    temp += self.gamma[j][i]\n",
    "                sum2.append(temp)\n",
    "#             print \"d\",              \n",
    "            for i in range(self.number_of_sources):\n",
    "                self.pi[i] = sum2[i]/len(self.X)\n",
    "            \n",
    "#             print \"e\",\n",
    "            for i in range(self.number_of_sources):\n",
    "                tot = np.zeros([self.D])\n",
    "                for j in range(self.N):\n",
    "                    a = np.array([self.X[j] - self.mu[i]])\n",
    "                    tot = np.add(tot,self.gamma[j][i] * np.square(a))\n",
    "                self.cov[i] = tot/(sum2[i]*1.0)\n",
    "            \n",
    "#             print \"f\",\n",
    "            for i in range(self.number_of_sources):\n",
    "                tot = np.zeros([self.D])\n",
    "                for j in range(self.N):\n",
    "                    tot += self.gamma[j][i] * self.X[j]\n",
    "                self.mu[i] = tot/sum2[i]\n",
    "                                \n",
    "#             print \"g\",\n",
    "#             print self.cov\n",
    "            \n",
    "            \n",
    "            hig=0.0\n",
    "            for i in range(len(self.X)):\n",
    "                chigg = 0.0\n",
    "                for j in range(self.number_of_sources):\n",
    "#                     print i,j,self.pi[j],len(self.mu[j]),len(self.cov[j]),len(self.X[i])\n",
    "                    chigg += self.pi[j] * prob(self.mu[j],self.cov[j],self.X[i])\n",
    "                hig += np.log(chigg)\n",
    "            self.log_likelihoods.append(hig)\n",
    "            \n",
    "#             print hig,\n",
    "            \n",
    "            try:\n",
    "                if(abs(self.log_likelihoods[-1]-self.log_likelihoods[-2])<self.tolerence):\n",
    "                    \"TOLERANCE REACHED........\"\n",
    "                    break\n",
    "            except:\n",
    "                pass\n",
    "            \"\"\"\n",
    "            This process of E step followed by a M step is now iterated a number of n times. In the second step for instance,\n",
    "            we use the calculated pi_new, mu_new and cov_new to calculate the new r_ic which are then used in the second M step\n",
    "            to calculat the mu_new2 and cov_new2 and so on....\n",
    "            \"\"\"\n",
    "    \n",
    "    def predict2(self,Y):\n",
    "        result = []\n",
    "        count = 0\n",
    "        for k in Y:\n",
    "            tot = 0.0\n",
    "            for i in range(self.number_of_sources):\n",
    "                m = self.mu[i]\n",
    "                c = self.cov[i]\n",
    "                tot += self.pi[i]*prob(m,c,k)\n",
    "            \n",
    "            result.append(np.log(tot))\n",
    "            count+=1\n",
    "        return np.array(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# GMM on 24-D colour histogram feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data1 = np.loadtxt('./training_2b_c1')\n",
    "train_data2 = np.loadtxt('./training_2b_c2')\n",
    "train_data3 = np.loadtxt('./training_2b_c3')\n",
    "total_train_data = np.loadtxt('./training_2b')\n",
    "\n",
    "nc=4\n",
    "ni=15\n",
    "to=0.00001\n",
    "acc=[]\n",
    "for w in range(6):\n",
    "    nc = pow(2,w)\n",
    "    print \"Gmm started....\",\n",
    "    g1=GMM(np.array(train_data1), nc,ni,to)\n",
    "    g1.run()\n",
    "    print \"Gmm 1 done....\",\n",
    "    g2=GMM(np.array(train_data2), nc,ni,to)\n",
    "    g2.run()\n",
    "    print \"Gmm 2 done....\",\n",
    "    g3=GMM(np.array(train_data3), nc,ni,to)\n",
    "    g3.run()\n",
    "    print \"Gmm 3 done....\",\n",
    "    \n",
    "    with open('2b_g1_'+str(nc),'wb') as handle:\n",
    "        pickle.dump(g1, handle)\n",
    "    with open('2b_g2_'+str(nc),'wb') as handle:\n",
    "        pickle.dump(g2, handle)\n",
    "    with open('2b_g3_'+str(nc),'wb') as handle:\n",
    "        pickle.dump(g3, handle)\n",
    "        \n",
    "    test_data1 = np.loadtxt('./test_2b_c1')\n",
    "    test_data2 = np.loadtxt('./test_2b_c2')\n",
    "    test_data3 = np.loadtxt('./test_2b_c3')\n",
    "    total_test_data = np.loadtxt('./test_2b')\n",
    "    def stat():\n",
    "        test_data_name = total_test_data\n",
    "        res1=g1.predict2(np.array(test_data_name))\n",
    "        res2=g2.predict2(np.array(test_data_name))\n",
    "        res3=g3.predict2(np.array(test_data_name))\n",
    "        def dbb():\n",
    "            ans=[]\n",
    "            for i in range(0,len(res1)):\n",
    "                val=1\n",
    "                temp=res1[i]\n",
    "                if(res2[i]>temp):\n",
    "                    val=2\n",
    "                    temp=res2[i]\n",
    "                if(res3[i]>temp):\n",
    "                    val=3\n",
    "                ans.append(val)\n",
    "            return ans\n",
    "        ans= dbb()\n",
    "        def conf_mat():\n",
    "            m=[]\n",
    "            count1=0\n",
    "            count12=0\n",
    "            count2=0\n",
    "            count21=0\n",
    "            count3=0\n",
    "            count31=0\n",
    "            for i in range(0, len(test_data1)):\n",
    "                if(ans[i]==1):\n",
    "                    count1 = count1+1;\n",
    "                if(ans[i]==2):\n",
    "                    count12 = count12+1;\n",
    "            for i in range(len(test_data1), len(test_data1)+len(test_data2)):\n",
    "                if(ans[i]==2):\n",
    "                    count2 = count2+1;\n",
    "                if(ans[i]==1):\n",
    "                    count21 = count21+1;\n",
    "            for i in range(len(test_data1)+len(test_data2), len(total_test_data)):\n",
    "                if(ans[i]==3):\n",
    "                    count3 = count3+1;\n",
    "                if(ans[i]==1):\n",
    "                    count31 = count31+1;\n",
    "           #    print len(test_data1), count1\n",
    "           #    print len(test_data2), count2\n",
    "            mm=[count1,count12,len(test_data1)-count1-count12]\n",
    "            m.append(mm)\n",
    "            mm=[count21, count2, len(test_data2)-count21-count2]\n",
    "            m.append(mm)\n",
    "            mm=[count31, len(test_data3)-count31-count3, count3]\n",
    "            m.append(mm)\n",
    "            return m\n",
    "        m=conf_mat()\n",
    "        print m\n",
    "        accuracy=(sum([m[i][i] for i in range(0,3)]))/(float)(sum([sum(m[i]) for i in range(0,3)]))\n",
    "        precision=[m[i][i]/(float)(m[0][i]+m[1][i]+m[2][i]) for i in range(0,3)]\n",
    "        recall=[m[i][i]/(float)(m[i][0]+m[i][1]+m[i][2]) for i in range(0,3)]\n",
    "        f_measure=[(2*precision[i]*recall[i])/(precision[i]+recall[i]) for i in range(0,3)]\n",
    "        mean_precision=sum([precision[i] for i in range(0,3)])/3.0\n",
    "        mean_recall=sum([recall[i] for i in range(0,3)])/3.0\n",
    "        mean_f_measure=sum([f_measure[i] for i in range(0,3)])/3.0\n",
    "        prior=[len(train_data1)/(float)(len(total_train_data)),len(train_data2)/(float)(len(total_train_data)),\n",
    "              len(train_data3)/(float)(len(total_train_data))]\n",
    "                       \n",
    "        print \"Prior : {0}\".format(prior)\n",
    "        print \"Confusion Matrix : {0}\".format(m)\n",
    "        print \"Accuracy :\",accuracy\n",
    "        print \"Precision :{0}\".format(precision),\"Mean Precision :\",mean_precision\n",
    "        print \"Recall :{0}\".format(recall),\"Mean Recall :\",mean_recall\n",
    "        print \"F-measure :{0}\".format(f_measure),\"Mean F-measure :\",mean_f_measure\n",
    "        acc.append([nc,accuracy]) \n",
    "        with open('2b_stats_'+str(nc),'wb') as handle:\n",
    "            handle.write(\"Prior : {0}\\n\".format(prior))\n",
    "            handle.write(\"Confusion Matrix : {0}\\n\".format(m))\n",
    "            handle.write(\"Accuracy : \"+str(accuracy)+\"\\n\")\n",
    "            handle.write(\"Precision :{0}\".format(precision)+\"\\nMean Precision : \"+str(mean_precision)+\"\\n\")\n",
    "            handle.write(\"Recall :{0}\".format(recall)+\"\\nMean Recall : \"+str(mean_recall)+\"\\n\")\n",
    "            handle.write(\"F-measure :{0}\".format(f_measure)+\"\\nMean F-measure : \"+str(mean_f_measure)+\"\\n\")\n",
    "    stat()\n",
    "with open('2b_24_acc','wb') as handle:\n",
    "    pickle.dump(acc, handle)\n",
    "print \"\\n\\n\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.mlab as mlab\n",
    "import pickle\n",
    "\n",
    "legend_facecolor='#f6ffc6'\n",
    "legend_edgecolor='#ff07ff'\n",
    "legend_linewidth=2\n",
    "legend_loc='upper right'\n",
    "\n",
    "c={}\n",
    "c[1]='#54bdff' # blue Class Color\n",
    "c[2]='#9ef442' # green\n",
    "c[3]='#FFB74D' # red\n",
    "\n",
    "c2={}\n",
    "c2[1]='#2205c4' # Training Points Color\n",
    "c2[2]='#388E3C'\n",
    "c2[3]='#ad061f'\n",
    "\n",
    "# matplotlib.style.use('seaborn')\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(14,14*0.89))\n",
    "for w in range(4):\n",
    "    nc = pow(2,w)\n",
    "    plt.subplot(221+w)\n",
    "    ax1.set_title(str(nc)+'-Cluster')\n",
    "    \n",
    "    with open('2b_g1_'+str(nc),'rb') as handle:\n",
    "        g1=pickle.loads(handle.read())\n",
    "        xx=np.linspace(0,100,len(g1.log_likelihoods))\n",
    "        plt.plot(xx,g1.log_likelihoods,c='r')\n",
    "    \n",
    "    with open('2b_g2_'+str(nc),'rb') as handle: \n",
    "        g2=pickle.loads(handle.read())\n",
    "        xx=np.linspace(0,100,len(g2.log_likelihoods))\n",
    "        plt.plot(xx,g2.log_likelihoods,c='g')\n",
    "        \n",
    "    with open('2b_g3_'+str(nc),'rb') as handle: \n",
    "        g3=pickle.loads(handle.read())\n",
    "        xx=np.linspace(0,100,len(g3.log_likelihoods))\n",
    "        plt.plot(xx,g3.log_likelihoods,c='b') \n",
    "\n",
    "    colors = ['r','g','b']\n",
    "    texts = [\"Class 1\", \"Class 2\", \"Class 3\"]\n",
    "    handles = [plt.Rectangle((0,0),1,1,color=colors[i]) for i  in range(len(texts))]\n",
    "    legend=fig.legend(handles=handles, labels=texts,loc='center', ncol=1,frameon=1, \n",
    "                      labelspacing=0.85, borderpad=0.25, prop=dict(weight='black',size='large'))\n",
    "    frame = legend.get_frame()\n",
    "    frame.set_facecolor(legend_facecolor)\n",
    "    frame.set_edgecolor(legend_edgecolor)\n",
    "    frame.set_linewidth(legend_linewidth)\n",
    "        #     plt.show()\n",
    "    \n",
    "    ad=ord('a')+w\n",
    "    plt.xlabel('('+chr(ad)+') '+str(nc)+'-Cluster',fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print acc\n",
    "X=[acc[i][0] for i in range(0,len(acc)) ]\n",
    "Y=[acc[i][1] for i in range(0,len(acc)) ]\n",
    "plt.plot(X,Y,c='b')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Accuracy')\n",
    "# plt.plot(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GMM on BoVW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data1 = np.loadtxt('./training_2b_c1_32')\n",
    "train_data2 = np.loadtxt('./training_2b_c2_32')\n",
    "train_data3 = np.loadtxt('./training_2b_c3_32')\n",
    "total_train_data = np.loadtxt('./training_2b_32')\n",
    "# g3=GMM(X3,4,20,0.001)\n",
    "# g3.run()\n",
    "\n",
    "nc=4\n",
    "ni=100\n",
    "to=0.00001\n",
    "acc=[]\n",
    "for w in range(6):\n",
    "    nc = pow(2,w)\n",
    "    print \"Gmm started....\",\n",
    "    g1=GMM(np.array(train_data1), nc,ni,to)\n",
    "    g1.run()\n",
    "    print \"Gmm 1 done....\",\n",
    "    g2=GMM(np.array(train_data2), nc,ni,to)\n",
    "    g2.run()\n",
    "    print \"Gmm 2 done....\",\n",
    "    g3=GMM(np.array(train_data3), nc,ni,to)\n",
    "    g3.run()\n",
    "    print \"Gmm 3 done....\",\n",
    "    \n",
    "    with open('2b_32_g1_'+str(nc),'wb') as handle:\n",
    "        pickle.dump(g1, handle)\n",
    "    with open('2b_32_g2_'+str(nc),'wb') as handle:\n",
    "        pickle.dump(g2, handle)\n",
    "    with open('2b_32_g3_'+str(nc),'wb') as handle:\n",
    "        pickle.dump(g3, handle)\n",
    "        \n",
    "    test_data1 = np.loadtxt('./test_2b_32_c1')\n",
    "    test_data2 = np.loadtxt('./test_2b_32_c2')\n",
    "    test_data3 = np.loadtxt('./test_2b_32_c3')\n",
    "    total_test_data = np.loadtxt('./test_2b_32')\n",
    "    def stat():\n",
    "        test_data_name = total_test_data\n",
    "        res1=g1.predict2(np.array(test_data_name))\n",
    "        res2=g2.predict2(np.array(test_data_name))\n",
    "        res3=g3.predict2(np.array(test_data_name))\n",
    "        def dbb():\n",
    "            ans=[]\n",
    "            for i in range(0,len(res1)):\n",
    "                val=1\n",
    "                temp=res1[i]\n",
    "                if(res2[i]>temp):\n",
    "                    val=2\n",
    "                    temp=res2[i]\n",
    "                if(res3[i]>temp):\n",
    "                    val=3\n",
    "                ans.append(val)\n",
    "            return ans\n",
    "        ans= dbb()\n",
    "        def conf_mat():\n",
    "            m=[]\n",
    "            count1=0\n",
    "            count12=0\n",
    "            count2=0\n",
    "            count21=0\n",
    "            count3=0\n",
    "            count31=0\n",
    "            for i in range(0, len(test_data1)):\n",
    "                if(ans[i]==1):\n",
    "                    count1 = count1+1;\n",
    "                if(ans[i]==2):\n",
    "                    count12 = count12+1;\n",
    "            for i in range(len(test_data1), len(test_data1)+len(test_data2)):\n",
    "                if(ans[i]==2):\n",
    "                    count2 = count2+1;\n",
    "                if(ans[i]==1):\n",
    "                    count21 = count21+1;\n",
    "            for i in range(len(test_data1)+len(test_data2), len(total_test_data)):\n",
    "                if(ans[i]==3):\n",
    "                    count3 = count3+1;\n",
    "                if(ans[i]==1):\n",
    "                    count31 = count31+1;\n",
    "           #    print len(test_data1), count1\n",
    "           #    print len(test_data2), count2\n",
    "            mm=[count1,count12,len(test_data1)-count1-count12]\n",
    "            m.append(mm)\n",
    "            mm=[count21, count2, len(test_data2)-count21-count2]\n",
    "            m.append(mm)\n",
    "            mm=[count31, len(test_data3)-count31-count3, count3]\n",
    "            m.append(mm)\n",
    "            return m\n",
    "        m=conf_mat()\n",
    "        print m\n",
    "        accuracy=(sum([m[i][i] for i in range(0,3)]))/(float)(sum([sum(m[i]) for i in range(0,3)]))\n",
    "        precision=[m[i][i]/(float)(m[0][i]+m[1][i]+m[2][i]) for i in range(0,3)]\n",
    "        recall=[m[i][i]/(float)(m[i][0]+m[i][1]+m[i][2]) for i in range(0,3)]\n",
    "        f_measure=[(2*precision[i]*recall[i])/(precision[i]+recall[i]) for i in range(0,3)]\n",
    "        mean_precision=sum([precision[i] for i in range(0,3)])/3.0\n",
    "        mean_recall=sum([recall[i] for i in range(0,3)])/3.0\n",
    "        mean_f_measure=sum([f_measure[i] for i in range(0,3)])/3.0\n",
    "        prior=[len(train_data1)/(float)(len(total_train_data)),len(train_data2)/(float)(len(total_train_data)),\n",
    "              len(train_data3)/(float)(len(total_train_data))]\n",
    "                       \n",
    "        print \"Prior : {0}\".format(prior)\n",
    "        print \"Confusion Matrix : {0}\".format(m)\n",
    "        print \"Accuracy :\",accuracy\n",
    "        print \"Precision :{0}\".format(precision),\"Mean Precision :\",mean_precision\n",
    "        print \"Recall :{0}\".format(recall),\"Mean Recall :\",mean_recall\n",
    "        print \"F-measure :{0}\".format(f_measure),\"Mean F-measure :\",mean_f_measure\n",
    "        acc.append([nc,accuracy]) \n",
    "        with open('2b_32_stats_'+str(nc),'wb') as handle:\n",
    "            handle.write(\"Prior : {0}\\n\".format(prior))\n",
    "            handle.write(\"Confusion Matrix : {0}\\n\".format(m))\n",
    "            handle.write(\"Accuracy : \"+str(accuracy)+\"\\n\")\n",
    "            handle.write(\"Precision :{0}\".format(precision)+\"\\nMean Precision : \"+str(mean_precision)+\"\\n\")\n",
    "            handle.write(\"Recall :{0}\".format(recall)+\"\\nMean Recall : \"+str(mean_recall)+\"\\n\")\n",
    "            handle.write(\"F-measure :{0}\".format(f_measure)+\"\\nMean F-measure : \"+str(mean_f_measure)+\"\\n\")\n",
    "    stat()\n",
    "with open('2b_acc','wb') as handle:\n",
    "    pickle.dump(acc, handle)\n",
    "print \"\\n\\n\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.mlab as mlab\n",
    "import pickle\n",
    "\n",
    "legend_facecolor='#f6ffc6'\n",
    "legend_edgecolor='#ff07ff'\n",
    "legend_linewidth=2\n",
    "legend_loc='upper right'\n",
    "\n",
    "c={}\n",
    "c[1]='#54bdff' # blue Class Color\n",
    "c[2]='#9ef442' # green\n",
    "c[3]='#FFB74D' # red\n",
    "\n",
    "c2={}\n",
    "c2[1]='#2205c4' # Training Points Color\n",
    "c2[2]='#388E3C'\n",
    "c2[3]='#ad061f'\n",
    "\n",
    "# matplotlib.style.use('seaborn')\n",
    "fig, ((ax1, ax2), (ax3, ax4), (ax5, ax6)) = plt.subplots(3, 2, figsize=(14,14*0.89))\n",
    "for w in range(5):\n",
    "    nc = pow(2,w)\n",
    "    plt.subplot(321+w)\n",
    "    ax1.set_title(str(nc)+'-Cluster')\n",
    "    \n",
    "    with open('2b_32_g1_'+str(nc),'rb') as handle:\n",
    "        g1=pickle.loads(handle.read())\n",
    "        xx=np.linspace(0,100,len(g1.log_likelihoods))\n",
    "        plt.plot(xx,g1.log_likelihoods,c='r')\n",
    "    \n",
    "    with open('2b_32_g2_'+str(nc),'rb') as handle: \n",
    "        g2=pickle.loads(handle.read())\n",
    "        xx=np.linspace(0,100,len(g2.log_likelihoods))\n",
    "        plt.plot(xx,g2.log_likelihoods,c='g')\n",
    "        \n",
    "    with open('2b_32_g3_'+str(nc),'rb') as handle: \n",
    "        g3=pickle.loads(handle.read())\n",
    "        xx=np.linspace(0,100,len(g3.log_likelihoods))\n",
    "        plt.plot(xx,g3.log_likelihoods,c='b') \n",
    "\n",
    "    colors = ['r','g','b']\n",
    "    texts = [\"Class 1\", \"Class 2\", \"Class 3\"]\n",
    "    handles = [plt.Rectangle((0,0),1,1,color=colors[i]) for i  in range(len(texts))]\n",
    "    legend=ax6.legend(handles=handles, labels=texts,loc='center', ncol=3,frameon=1, \n",
    "                      labelspacing=1.5, borderpad=0.75, prop=dict(weight='black',size='large'))\n",
    "    frame = legend.get_frame()\n",
    "    frame.set_facecolor(legend_facecolor)\n",
    "    frame.set_edgecolor(legend_edgecolor)\n",
    "    frame.set_linewidth(legend_linewidth)\n",
    "        #     plt.show()\n",
    "    \n",
    "    ad=ord('a')+w\n",
    "    plt.xlabel('('+chr(ad)+') '+str(nc)+'-Cluster',fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print acc\n",
    "X=[acc[i][0] for i in range(0,len(acc)) ]\n",
    "Y=[acc[i][1] for i in range(0,len(acc)) ]\n",
    "plt.plot(X,Y,c='b')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Accuracy')\n",
    "# plt.plot(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
